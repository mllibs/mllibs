{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c991d33-26f6-40a1-9365-f4bda2437d07",
   "metadata": {},
   "source": [
    "## **Named Entity Recognition** \n",
    "\n",
    "### **NER Parser**\n",
    "\n",
    "Create NER tagger to identify words/tokens of interest in input request, it is used to set parameters & remove irrelovant tokens before feeding the input into the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4131af7-ea30-466c-895e-10ce258e967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import regex as re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "'''\n",
    "\n",
    "PARSER FOR THE DATASET NER TAG FORMAT\n",
    "\n",
    "'''\n",
    "\n",
    "class Parser:\n",
    "    \n",
    "    # RE patterns for tag extraction\n",
    "    LABEL_PATTERN = r\"\\[(.*?)\\]\"\n",
    "    PUNCTUATION_PATTERN = r\"([,\\/#!$%\\^&\\*;:{}=\\-`~()'\\\"’¿])\"\n",
    "    \n",
    "    \n",
    "    # initialise, first word/id tag is O (outside)\n",
    "    def __init__(self):\n",
    "        self.tag_to_id = {\n",
    "            \"O\": 0\n",
    "        }\n",
    "        self.id_to_tag = {\n",
    "            0: \"O\"\n",
    "        }\n",
    "        \n",
    "    ''' CREATE TAGS '''\n",
    "        \n",
    "    # input : sentence, tagged sentence\n",
    "        \n",
    "    def __call__(self, sentence: str, annotated: str) -> List[str]:\n",
    "        \n",
    "        ''' Create Dictionary of Identified Tags'''\n",
    "        \n",
    "        # 1. set label B or I    \n",
    "        matches = re.findall(self.LABEL_PATTERN, annotated)\n",
    "        word_to_tag = {}\n",
    "        \n",
    "        for match in matches:            \n",
    "            if(\" : \" in match):\n",
    "                tag, phrase = match.split(\" : \")\n",
    "                words = phrase.split(\" \") \n",
    "                word_to_tag[words[0]] = f\"B-{tag.upper()}\"\n",
    "                for w in words[1:]:\n",
    "                    word_to_tag[w] = f\"I-{tag.upper()}\"\n",
    "                \n",
    "        ''' Tokenise Sentence & add tags to not tagged words (O)'''\n",
    "                \n",
    "        # 2. add token tag to main tag dictionary\n",
    "\n",
    "        tags = []\n",
    "        sentence = re.sub(self.PUNCTUATION_PATTERN, r\" \\1 \", sentence)\n",
    "        \n",
    "        for w in sentence.split():\n",
    "            if w not in word_to_tag:\n",
    "                tags.append(\"O\")\n",
    "            else:\n",
    "                tags.append(word_to_tag[w])\n",
    "                self.__add_tag(word_to_tag[w])\n",
    "                \n",
    "        return tags\n",
    "    \n",
    "    ''' TAG CONVERSION '''\n",
    "    \n",
    "    # to word2id (tag_to_id)\n",
    "    # to id2word (id_to_tag)\n",
    "\n",
    "    def __add_tag(self, tag: str):\n",
    "        if tag in self.tag_to_id:\n",
    "            return\n",
    "        id_ = len(self.tag_to_id)\n",
    "        self.tag_to_id[tag] = id_\n",
    "        self.id_to_tag[id_] = tag\n",
    "        \n",
    "        ''' Get Tag Number ID '''\n",
    "        # or just number id for token\n",
    "        \n",
    "    def get_id(self, tag: str):\n",
    "        return self.tag_to_id[tag]\n",
    "    \n",
    "    ''' Get Tag Token from Number ID'''\n",
    "    # given id get its token\n",
    "    \n",
    "    def get_label(self, id_: int):\n",
    "        return self.get_tag_label(id_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ee2548-9dda-493a-bdd5-89cd0aa8947a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/miniconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.973\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     B-PARAM       0.99      1.00      0.99        81\n",
      "        B-PP       1.00      1.00      1.00         9\n",
      "    B-SOURCE       0.81      1.00      0.90        39\n",
      "    I-SOURCE       0.83      1.00      0.91         5\n",
      "           O       1.00      0.96      0.98       274\n",
      "\n",
      "    accuracy                           0.97       408\n",
      "   macro avg       0.93      0.99      0.96       408\n",
      "weighted avg       0.98      0.97      0.97       408\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B-PARAM</th>\n",
       "      <th>B-PP</th>\n",
       "      <th>B-SOURCE</th>\n",
       "      <th>I-SOURCE</th>\n",
       "      <th>O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B-PARAM</th>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PP</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-SOURCE</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-SOURCE</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          B-PARAM  B-PP  B-SOURCE  I-SOURCE    O\n",
       "B-PARAM        81     0         0         0    0\n",
       "B-PP            0     9         0         0    0\n",
       "B-SOURCE        0     0        39         0    0\n",
       "I-SOURCE        0     0         0         5    0\n",
       "O               1     0         9         1  263"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "NER with Machine Learning Models\n",
    "\n",
    "'''\n",
    "    \n",
    "# pattern for tokenisation\n",
    "PUNCTUATION_PATTERN = r\"([,\\/#!$%\\^&\\*;:{}=\\-`~()'\\\"’¿])\"\n",
    "\n",
    "# customiser tokeniser\n",
    "def cust_tokeniser(inputs):\n",
    "    sentence = re.sub(PUNCTUATION_PATTERN, r\" \\1 \", inputs)\n",
    "    return sentence.split()\n",
    "\n",
    "# parser\n",
    "parser = Parser()\n",
    "df = pd.read_csv('ner_modelparams_annot.csv')   # read dataframe\n",
    "\n",
    "def make_model(parser,df):\n",
    "\n",
    "    # parse our NER tag data & tokenise our text\n",
    "    lst_data = []; lst_tags = []\n",
    "    for ii,row in df.iterrows():\n",
    "        sentence = re.sub(PUNCTUATION_PATTERN, r\" \\1 \", row['text'])\n",
    "        lst_data.extend(sentence.split())\n",
    "        lst_tags.extend(parser(row[\"text\"], row[\"annot\"]))\n",
    "    \n",
    "    ldf = pd.DataFrame({'data':lst_data,\n",
    "                        'tag':lst_tags})\n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    Vectorisation \n",
    "    \n",
    "    '''\n",
    "        \n",
    "    # define encoder\n",
    "    # encoder = CountVectorizer(tokenizer=cust_tokeniser,ngram_range=(1,1))\n",
    "    encoder = CountVectorizer(tokenizer=cust_tokeniser)\n",
    "    # encoder = TfidfVectorizer(tokenizer=cust_tokeniser,ngram_range=(1,5))\n",
    "    X = encoder.fit_transform(lst_data)\n",
    "    y = np.array(lst_tags)\n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    Modeling \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # try our different models\n",
    "    # model_confirm = LogisticRegression()\n",
    "    model_confirm = CatBoostClassifier(silent=True)\n",
    "    # model_confirm = RandomForestClassifier(max_depth=200,min_samples_split=10)\n",
    "    \n",
    "    # train model\n",
    "    model_confirm.fit(X,y)\n",
    "    y_pred = model_confirm.predict(X)\n",
    "    print(f'accuracy: {round(accuracy_score(y_pred,y),3)}')\n",
    "\n",
    "    print(classification_report(y, y_pred))\n",
    "    display(pd.DataFrame(confusion_matrix(y,y_pred),index=model_confirm.classes_,columns=model_confirm.classes_))\n",
    "    return model_confirm,encoder\n",
    "\n",
    "model,encoder = make_model(parser,df)\n",
    "# df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2843ea96-4a0b-4693-9f00-ab3df268b066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>show</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unique</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>values</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>columns</td>\n",
       "      <td>B-PARAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>island</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>data</td>\n",
       "      <td>I-SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>penguins</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      input      pred\n",
       "0      show         O\n",
       "1       the         O\n",
       "2    unique         O\n",
       "3    values         O\n",
       "4        in         O\n",
       "5   columns   B-PARAM\n",
       "6    island         O\n",
       "7        in         O\n",
       "8      data  I-SOURCE\n",
       "9  penguins         O"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs = \"create scatterplot using data and x A y B and hue C\"\n",
    "# inputs = \"create relplot using data x flow, y length col:A and row D, alpha 0.1\"\n",
    "# inputs1 = \"create seaborn scatterplot using data penguins x bill_length_mm y bill_depth_mm hue island\"\n",
    "# inputs2 = \"create seaborn scatterplot using penguins x bill_length_mm y bill_depth_mm hue island\"\n",
    "# inputs = \"create seaborn scatterplot using data penguins x bill_length_mm y bill_depth_mm hue island select numerical features only\"\n",
    "# inputs = \"create seaborn scatterplot using data penguins (use numerical columns only) x bill_length_mm y bill_depth_mm hue island\"\n",
    "\n",
    "import itertools\n",
    "\n",
    "'''\n",
    "\n",
    "Implementing references to dataframe subsets\n",
    "\n",
    "'''\n",
    "\n",
    "# inputs = \"create label encoding of column B using data A\"     # not ok\n",
    "# inputs = \"create label encoding for column B using data A\"    # not ok\n",
    "# inputs = \"create one hot encoding of columns A B C using data E\" # ok\n",
    "# inputs = \"create label encoding using active columns C from data E\"\n",
    "inputs = \"show the unique values in columns island in data penguins\"\n",
    "\n",
    "# predict NER tags\n",
    "def ner_predict(inputs):\n",
    "    # tokens = word_tokenize(inputs)\n",
    "    tokens = cust_tokeniser(inputs)\n",
    "    y_pred_test = model.predict(encoder.transform(tokens))\n",
    "    y_pred_test = list(itertools.chain.from_iterable(y_pred_test))\n",
    "    return pd.DataFrame({\"input\":tokens,\n",
    "                         \"pred\":y_pred_test})\n",
    "\n",
    "\n",
    "outputs = ner_predict(inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "61c429af-b33a-443a-8888-f1edb77991b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk import pos_tag\n",
    "# from nltk.chunk import ne_chunk\n",
    "# from nltk.corpus import treebank\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# # Sample text data\n",
    "# text = \"Apple is planning to open a new store in San Francisco next month.\"\n",
    "\n",
    "# # Tokenize the input text\n",
    "# tokens = word_tokenize(text)\n",
    "\n",
    "# # Perform part-of-speech tagging\n",
    "# pos_tags = pos_tag(tokens)\n",
    "\n",
    "# # Define a simple rule-based named entity recognition function\n",
    "# def extract_named_entities(pos_tags):\n",
    "#     named_entities = []\n",
    "#     for chunk in ne_chunk(pos_tags):\n",
    "#         if hasattr(chunk, 'label'):\n",
    "#             entity = ' '.join(c[0] for c in chunk)\n",
    "#             named_entities.append((entity, chunk.label()))\n",
    "#     return named_entities\n",
    "\n",
    "# # Extract named entities from the text using the rule-based function\n",
    "# named_entities = extract_named_entities(pos_tags)\n",
    "\n",
    "# # Define features for each word/token\n",
    "# def word_features(word, index, tokens):\n",
    "#     return {\n",
    "#         'word': word,\n",
    "#         'is_first': index == 0,\n",
    "#         'is_last': index == len(tokens) - 1,\n",
    "#         'is_title': word.istitle(),\n",
    "#         'is_upper': word.isupper(),\n",
    "#         'is_lower': word.islower(),\n",
    "#         'prefix-1': word[0],\n",
    "#         'prefix-2': word[:2],\n",
    "#         'suffix-1': word[-1],\n",
    "#         'suffix-2': word[-2:],\n",
    "#     }\n",
    "\n",
    "# # Extract features for each token in the text\n",
    "# features = [word_features(token, i, tokens) for i, token in enumerate(tokens)]\n",
    "\n",
    "# # Convert features to a sparse matrix using DictVectorizer\n",
    "# vectorizer = DictVectorizer(sparse=True)\n",
    "# X = vectorizer.fit_transform(features)\n",
    "\n",
    "# # Define labeled named entities for training\n",
    "# y = ['O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'B-DATE']\n",
    "\n",
    "# # Train a Random Forest model\n",
    "# clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# clf.fit(X, y)\n",
    "\n",
    "# # New test text for prediction\n",
    "# test_text = \"Microsoft is also considering a new office in Seattle next year.\"\n",
    "\n",
    "# # Tokenize and extract features for the test text\n",
    "# test_tokens = word_tokenize(test_text)\n",
    "# test_features = [word_features(token, i, test_tokens) for i, token in enumerate(test_tokens)]\n",
    "\n",
    "# # Use the same DictVectorizer object to transform the test features\n",
    "# X_test = vectorizer.transform(test_features)\n",
    "\n",
    "# # Make predictions using the trained Random Forest model\n",
    "# predictions = clf.predict(X_test)\n",
    "\n",
    "# # Print the predicted named entities\n",
    "# print(list(zip(test_tokens, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "663aec6f-7737-46a8-ae05-826024821ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Sample data\n",
    "# data = {\n",
    "#     'text': ['Apple is a company', 'New York is a city', 'John works at Google'],\n",
    "#     'label': ['ORG', 'LOC', 'PER']\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Feature extraction\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(df['text'])\n",
    "# y = df['label']\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train the Random Forest classifier\n",
    "# clf = RandomForestClassifier()\n",
    "# clf.fit(X, y)\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred = clf.predict(X)\n",
    "# y_pred\n",
    "\n",
    "# # Evaluate the model\n",
    "# # print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "228ae0c4-927b-4ba9-b56c-cbfea825305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "\n",
    "# Use Transformer Embedding\n",
    "\n",
    "# '''\n",
    "\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load pre-trained BERT model and tokenizer\n",
    "# model_name = \"bert-base-uncased\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# # Example text and corresponding labels\n",
    "# texts = [\"Apple is a company based in California.\", \"Python is a programming language.\"]\n",
    "# labels = [\"ORG\", \"MISC\"]\n",
    "\n",
    "# # Tokenize and encode the texts\n",
    "# encoded_texts = [tokenizer.encode(text, return_tensors=\"pt\", padding=True, truncation=True) for text in texts]\n",
    "\n",
    "# # Generate BERT embeddings for the encoded texts\n",
    "# with torch.no_grad():\n",
    "#     outputs = [model(input_ids).last_hidden_state.mean(dim=1).numpy() for input_ids in encoded_texts]\n",
    "\n",
    "# # Flatten the embeddings and labels for training\n",
    "# X = np.concatenate(outputs, axis=0)\n",
    "# y = labels\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train a Random Forest classifier\n",
    "# clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the classifier\n",
    "# accuracy = clf.score(X_test, y_test)\n",
    "# print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "22936391-1c31-4d24-ad50-cbeafb778bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk import word_tokenize\n",
    "# from nltk.util import ngrams\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# # Sample dataset with labeled texts\n",
    "# texts = [\n",
    "#     \"Apple is a company based in California.\",\n",
    "#     \"Microsoft is known for its Windows operating system.\",\n",
    "#     \"Python is a popular programming language.\",\n",
    "#     \"The Eiffel Tower is located in Paris, France.\"\n",
    "# ]\n",
    "# labels = [\"ORG\", \"ORG\", \"MISC\", \"LOC\"]\n",
    "\n",
    "# # Function to extract n-gram features from the input text\n",
    "# def extract_ngram_features(text, n):\n",
    "#     tokens = word_tokenize(text)\n",
    "#     ngram_features = ngrams(tokens, n)\n",
    "#     return [' '.join(gram) for gram in ngram_features]\n",
    "\n",
    "# # Extract n-gram features for each text\n",
    "# n = 2  # Using bigram features\n",
    "# ngram_features = [extract_ngram_features(text, n) for text in texts]\n",
    "\n",
    "# # Convert n-gram features to dictionary format for vectorization\n",
    "# ngram_features_dict = [{feature: 1 for feature in features} for features in ngram_features]\n",
    "\n",
    "# # Vectorize the n-gram features\n",
    "# vectorizer = DictVectorizer()\n",
    "# X = vectorizer.fit_transform(ngram_features_dict)\n",
    "# y = labels\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train a Random Forest classifier\n",
    "# clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# clf.fit(X, y)\n",
    "\n",
    "# # Predict labels for the test set\n",
    "# y_pred = clf.predict(X)\n",
    "\n",
    "# # Evaluate the classifier\n",
    "# accuracy = accuracy_score(y, y_pred)\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# # Print classification report\n",
    "# print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "34bf19c5-8a9e-46ca-9300-5137da061737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk import word_tokenize\n",
    "# from nltk.util import ngrams\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# # Sample dataset with labeled texts\n",
    "# texts = [\n",
    "#     \"Apple is a company based in California.\",\n",
    "#     \"Microsoft is known for its Windows operating system.\",\n",
    "#     \"Python is a popular programming language.\",\n",
    "#     \"The Eiffel Tower is located in Paris, France.\"\n",
    "# ]\n",
    "# labels = [\"ORG\", \"ORG\", \"MISC\", \"LOC\"]\n",
    "\n",
    "# # Function to extract n-gram features from the input text\n",
    "# def extract_ngram_features(text, n):\n",
    "#     tokens = word_tokenize(text)\n",
    "#     ngram_features = ngrams(tokens, n)\n",
    "#     return [' '.join(gram) for gram in ngram_features]\n",
    "\n",
    "# # Extract unigram and bigram features for each text\n",
    "# unigram_features = [word_tokenize(text) for text in texts]\n",
    "# bigram_features = [extract_ngram_features(text, 2) for text in texts]\n",
    "\n",
    "# # # Convert unigram and bigram features to dictionary format for vectorization\n",
    "# unigram_features_dict = [{feature: 1 for feature in features} for features in unigram_features]\n",
    "# bigram_features_dict = [{feature: 1 for feature in features} for features in bigram_features]\n",
    "\n",
    "# # # Combine unigram and bigram features\n",
    "# combined_features_dict = [{**uni, **bi} for uni, bi in zip(unigram_features_dict, bigram_features_dict)]\n",
    "# combined_features_dict\n",
    "\n",
    "# # # Vectorize the combined features\n",
    "# # vectorizer = DictVectorizer()\n",
    "# # X = vectorizer.fit_transform(combined_features_dict)\n",
    "# # print(X.shape)\n",
    "\n",
    "# # # Split the data into training and testing sets\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # # Train a Random Forest classifier\n",
    "# # clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# # clf.fit(X_train, y_train)\n",
    "\n",
    "# # # Predict labels for the test set\n",
    "# # y_pred = clf.predict(X_test)\n",
    "\n",
    "# # # Evaluate the classifier\n",
    "# # accuracy = accuracy_score(y_test, y_pred)\n",
    "# # print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# # # Print classification report\n",
    "# # print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2bb57b-88e8-4921-927c-72f119bb4384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import pandas as pd    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "'''\n",
    "\n",
    "PARSER FOR THE DATASET NER TAG FORMAT\n",
    "\n",
    "'''\n",
    "\n",
    "# Tokenisation patten\n",
    "PUNCTUATION_PATTERN = r\"([,\\/#!$%\\^&\\*;:{}=\\-`~()'\\\"’¿])\"\n",
    "# RE patterns for tag extraction\n",
    "LABEL_PATTERN = r\"\\[(.*?)\\]\"\n",
    "\n",
    "class Parser:\n",
    "    \n",
    "    # initialise, first word/id tag is O (outside)\n",
    "    def __init__(self):\n",
    "        self.tag_to_id = {\"O\": 0}\n",
    "        self.id_to_tag = {0: \"O\"}\n",
    "        \n",
    "    ''' CREATE TAGS '''\n",
    "        \n",
    "    # input : sentence, tagged sentence\n",
    "        \n",
    "    def __call__(self, sentence: str, annotated: str) -> List[str]:\n",
    "        \n",
    "        ''' Create Dictionary of Identified Tags'''\n",
    "        \n",
    "        # 1. set label B or I    \n",
    "        matches = re.findall(LABEL_PATTERN, annotated)\n",
    "        word_to_tag = {}\n",
    "        \n",
    "        for match in matches:            \n",
    "            if(\" : \" in match):\n",
    "                tag, phrase = match.split(\" : \")\n",
    "                words = phrase.split(\" \") \n",
    "                word_to_tag[words[0]] = f\"B-{tag.upper()}\"\n",
    "                for w in words[1:]:\n",
    "                    word_to_tag[w] = f\"I-{tag.upper()}\"\n",
    "                \n",
    "        ''' Tokenise Sentence & add tags to not tagged words (O)'''\n",
    "                \n",
    "        # 2. add token tag to main tag dictionary\n",
    "\n",
    "        tags = []\n",
    "        sentence = re.sub(PUNCTUATION_PATTERN, r\" \\1 \", sentence)\n",
    "        \n",
    "        for w in sentence.split():\n",
    "            if w not in word_to_tag:\n",
    "                tags.append(\"O\")\n",
    "            else:\n",
    "                tags.append(word_to_tag[w])\n",
    "                self.__add_tag(word_to_tag[w])\n",
    "                \n",
    "        return tags\n",
    "    \n",
    "    ''' TAG CONVERSION '''\n",
    "    \n",
    "    # to word2id (tag_to_id)\n",
    "    # to id2word (id_to_tag)\n",
    "\n",
    "    def __add_tag(self, tag: str):\n",
    "        if tag in self.tag_to_id:\n",
    "            return\n",
    "        id_ = len(self.tag_to_id)\n",
    "        self.tag_to_id[tag] = id_\n",
    "        self.id_to_tag[id_] = tag\n",
    "        \n",
    "        ''' Get Tag Number ID '''\n",
    "        # or just number id for token\n",
    "        \n",
    "    def get_id(self, tag: str):\n",
    "        return self.tag_to_id[tag]\n",
    "    \n",
    "    ''' Get Tag Token from Number ID'''\n",
    "    # given id get its token\n",
    "    \n",
    "    def get_label(self, id_: int):\n",
    "        return self.get_tag_label(id_)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "parser = Parser()\n",
    "df = pd.read_csv('ner_corpus.csv',delimiter=',')\n",
    "\n",
    "'''\n",
    "\n",
    "Create NER corpus\n",
    "\n",
    "'''\n",
    "\n",
    "from nltk.tokenize import word_tokenize, WhitespaceTokenizer \n",
    "\n",
    "def make_ner_corpus(parser,df:pd.DataFrame):\n",
    "\n",
    "    # parse our NER tag data & tokenise our text\n",
    "    lst_data = []; lst_tags = []\n",
    "    for ii,row in df.iterrows():\n",
    "        sentence = re.sub(PUNCTUATION_PATTERN, r\" \\1 \", row['question'])\n",
    "        lst_data.extend(sentence.split())\n",
    "        lst_tags.extend(parser(row[\"question\"], row[\"annotated\"]))\n",
    "        \n",
    "    return lst_data,lst_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32357cb8-7154-4b60-be77-9f2e690f8985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSmaller Variant\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Full Variant\n",
    "\n",
    "'''\n",
    "\n",
    "def extract_token_features2(tokens: list):\n",
    "    token_features = []\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        features = {\n",
    "            'token': token,\n",
    "            'is_first_token': i == 0,\n",
    "            'is_last_token': i == len(tokens) - 1,\n",
    "            'is_capitalized': token[0].isupper(),\n",
    "            'is_alphanumeric': token.isalnum(),\n",
    "            'is_punctuation': token in punctuation\n",
    "        }\n",
    "\n",
    "        if i < len(tokens) - 1:\n",
    "            next_token = tokens[i+1]\n",
    "            features['next_token_p1'] = next_token\n",
    "            features['is_next_first_token_p1'] = i + 1 == 0\n",
    "            features['is_next_last_token_p1'] = i + 1 == len(tokens) - 2\n",
    "            features['is_next_numeric_p1'] = next_token.isdigit()\n",
    "            features['is_next_alphanumeric_p1'] = next_token.isalnum()\n",
    "            features['is_next_punctuation_p1'] = next_token in punctuation\n",
    "        else:\n",
    "            features['next_token_p1'] = None\n",
    "            features['is_next_first_token_p1'] = False\n",
    "            features['is_next_last_token_p1'] = False\n",
    "            features['is_next_numeric_p1'] = False\n",
    "            features['is_next_alphanumeric_p1'] = False\n",
    "            features['is_next_punctuation_p1'] = False\n",
    "        \n",
    "        if i > 1:\n",
    "            prev_token = tokens[i-1]\n",
    "            features['prev_token_m1'] = prev_token\n",
    "            features['is_prev_first_token_m1'] = i - 1 == 0\n",
    "            features['is_prev_last_token_m1'] = i - 1 == len(tokens) - 2\n",
    "            features['is_prev_numeric_m1'] = prev_token.isdigit()\n",
    "            features['is_prev_alphanumeric_m1'] = prev_token.isalnum()\n",
    "            features['is_prev_punctuation_m1'] = prev_token in punctuation\n",
    "        else:\n",
    "            features['prev_token_m1'] = None\n",
    "            features['is_prev_first_token_m1'] = False\n",
    "            features['is_prev_last_token_m1'] = False\n",
    "            features['is_prev_numeric_m1'] = False\n",
    "            features['is_prev_alphanumeric_m1'] = False\n",
    "            features['is_prev_punctuation_1'] = False\n",
    "\n",
    "        if i < len(tokens) - 2:\n",
    "            next_token = tokens[i+2]\n",
    "            features['next_token_p2'] = next_token\n",
    "            features['is_next_first_token_p2'] = i + 1 == 0\n",
    "            features['is_next_last_token_p2'] = i + 1 == len(tokens) - 2\n",
    "            features['is_next_numeric_p2'] = next_token.isdigit()\n",
    "            features['is_next_alphanumeric_p2'] = next_token.isalnum()\n",
    "            features['is_next_punctuation_p2'] = next_token in punctuation\n",
    "        else:\n",
    "            features['next_token_p2'] = None\n",
    "            features['is_next_first_token_p2'] = False\n",
    "            features['is_next_last_token_p2'] = False\n",
    "            features['is_next_numeric_p2'] = False\n",
    "            features['is_next_alphanumeric_p2'] = False\n",
    "            features['is_next_punctuation_p2'] = False\n",
    "\n",
    "        if i > 2:\n",
    "            prev_token = tokens[i-2]\n",
    "            features['prev_token_m2'] = prev_token\n",
    "            features['is_prev_first_token_m2'] = i - 1 == 0\n",
    "            features['is_prev_last_token_m2'] = i - 1 == len(tokens) - 2\n",
    "            features['is_prev_numeric_m2'] = prev_token.isdigit()\n",
    "            features['is_prev_alphanumeric_m2'] = prev_token.isalnum()\n",
    "            features['is_prev_punctuation_m2'] = prev_token in punctuation\n",
    "        else:\n",
    "            features['prev_token_m2'] = None\n",
    "            features['is_prev_first_token_m2'] = False\n",
    "            features['is_prev_last_token_m2'] = False\n",
    "            features['is_prev_numeric_m2'] = False\n",
    "            features['is_prev_alphanumeric_m2'] = False\n",
    "            features['is_prev_punctuation_m2'] = False\n",
    "\n",
    "        token_features.append(features)\n",
    "        \n",
    "    return token_features\n",
    "\n",
    "'''\n",
    "\n",
    "Smaller Variant\n",
    "\n",
    "'''\n",
    "\n",
    "# def extract_token_features(tokens:list):\n",
    "    \n",
    "#     token_features = []\n",
    "#     for i, token in enumerate(tokens):\n",
    "#         features = {\n",
    "#             'token': token,\n",
    "#             'is_first_token': i == 0,\n",
    "#             'is_last_token': i == len(tokens) - 1,\n",
    "#             'is_capitalized': token[0].isupper(),\n",
    "#             'is_all_caps': token.isupper(),\n",
    "#             'is_numeric': token.isdigit(),\n",
    "#             'is_alphanumeric': token.isalnum(),\n",
    "#             'is_punctuation': token in punctuation\n",
    "#         }\n",
    "#         token_features.append(features)\n",
    "        \n",
    "#     return token_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d47ddfd5-b6c5-4c48-a855-031b53551cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.998\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     B-PARAM       1.00      1.00      1.00       101\n",
      "        B-PP       1.00      1.00      1.00         3\n",
      "    B-SOURCE       1.00      0.98      0.99        47\n",
      "     I-PARAM       1.00      1.00      1.00         6\n",
      "    I-SOURCE       1.00      1.00      1.00        13\n",
      "           O       1.00      1.00      1.00       293\n",
      "\n",
      "    accuracy                           1.00       463\n",
      "   macro avg       1.00      1.00      1.00       463\n",
      "weighted avg       1.00      1.00      1.00       463\n",
      "\n",
      "[[101   0   0   0   0   0]\n",
      " [  0   3   0   0   0   0]\n",
      " [  0   0  46   0   0   1]\n",
      " [  0   0   0   6   0   0]\n",
      " [  0   0   0   0  13   0]\n",
      " [  0   0   0   0   0 293]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('using', 'B-SOURCE', array(['O'], dtype=object))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from string import punctuation\n",
    "from catboost import CatBoostClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset with labeled tokens\n",
    "# tokens = [\"bos\",\"Apple\",\"is\",\"a\",\"company\",\"based\",\"in\",\"California\",\".\",\"eos\"]\n",
    "# labels = [\"ST\",\"ORG\", \"O\", \"O\", \"O\", \"O\", \"O\", \"LOC\", \"O\",\"ST\"]\n",
    "\n",
    "'''\n",
    "############################################################\n",
    "\n",
    "tf-idf transformer approach to NER\n",
    "\n",
    "        need to tokenise first; use whitespace tokeniser\n",
    "        so its the same as the dicttransformer\n",
    "\n",
    "############################################################\n",
    "'''\n",
    "\n",
    "from nltk.tokenize import word_tokenize, WhitespaceTokenizer \n",
    "\n",
    "def nltk_wtokeniser(text):\n",
    "    return WhitespaceTokenizer().tokenize(text)\n",
    "\n",
    "def tfidf(tokens:list,vectoriser=None):\n",
    "    \n",
    "    # tokenise by whitespaces (include dots)\n",
    "    if(vectoriser == None):\n",
    "        vectoriser = TfidfVectorizer(tokenizer=lambda x: nltk_wtokeniser(x),token_pattern=None)\n",
    "        X = vectoriser.fit_transform(tokens)\n",
    "        return X,vectoriser\n",
    "    else:\n",
    "        X = vectoriser.transform(tokens)\n",
    "        return X,_\n",
    "\n",
    "'''\n",
    "############################################################\n",
    "\n",
    "dicttransformers approach to NER\n",
    "\n",
    "        created for each token in list\n",
    "\n",
    "############################################################\n",
    "'''\n",
    "\n",
    "def dicttransformer(tokens:list,vectoriser=None):\n",
    "\n",
    "    # Extract token-level features for each token\n",
    "    token_features = extract_token_features2(tokens)\n",
    "    \n",
    "    # Vectorize the token features\n",
    "    if(vectoriser == None):\n",
    "        vectoriser = DictVectorizer()\n",
    "        X = vectoriser.fit_transform(token_features) # also sparse\n",
    "        return X,vectoriser\n",
    "    else:\n",
    "        X = vectoriser.transform(token_features) # also sparse\n",
    "        return X,_\n",
    "        \n",
    "    \n",
    "\n",
    "    return X,vectoriser\n",
    "\n",
    "'''\n",
    "\n",
    "Merge and Predict\n",
    "\n",
    "'''\n",
    "\n",
    "# merge tf-idf & dict features & train model\n",
    "def merger_train(X1,X2,y):\n",
    "\n",
    "    # convert to non-sparse \n",
    "    X_vect1 = pd.DataFrame(np.asarray(X1.todense()))\n",
    "    X_vect2 = pd.DataFrame(np.asarray(X2.todense()))\n",
    "    data = pd.concat([X_vect1,X_vect2],axis=1)\n",
    "    data = data.values\n",
    "\n",
    "    model = CatBoostClassifier(silent=True)\n",
    "    # model = LogisticRegression()\n",
    "    # model = RandomForestClassifier()\n",
    "    model.fit(data,y)\n",
    "    return data,model\n",
    "\n",
    "# merge tf-idf & dict features & train model\n",
    "def merger(X1,X2):\n",
    "\n",
    "    # convert to non-sparse \n",
    "    X_vect1 = pd.DataFrame(np.asarray(X1.todense()))\n",
    "    X_vect2 = pd.DataFrame(np.asarray(X2.todense()))\n",
    "    data = pd.concat([X_vect1,X_vect2],axis=1)\n",
    "    data = data.values # convert to numpy\n",
    "\n",
    "    return data\n",
    "\n",
    "# predict & measure metric\n",
    "def predict_label(X,tokens,labels,model):\n",
    "    y_pred = model.predict(X)\n",
    "    accuracy = accuracy_score(labels, y_pred)\n",
    "    print(f'accuracy: {round(accuracy_score(y_pred,labels),3)}')\n",
    "    print(classification_report(labels, y_pred))\n",
    "    print(confusion_matrix(labels,y_pred))\n",
    "    # display(pd.DataFrame({'y':tokens,\n",
    "    #                       'yp':list(itertools.chain(*y_pred))}).T)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def predict_label(X, tokens, labels, model):\n",
    "    y_pred = model.predict(X)\n",
    "    mispredictions = []\n",
    "    \n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] != labels[i]:\n",
    "            mispredictions.append((tokens[i], labels[i], y_pred[i]))\n",
    "    \n",
    "    accuracy = accuracy_score(labels, y_pred)\n",
    "    print(f'accuracy: {round(accuracy_score(y_pred, labels), 3)}')\n",
    "    print(classification_report(labels, y_pred))\n",
    "    print(confusion_matrix(labels, y_pred))\n",
    "    return mispredictions\n",
    "\n",
    "import itertools\n",
    "\n",
    "# just predict (inference)\n",
    "def predict(X,tokens,model):\n",
    "    y_pred = model.predict(X)\n",
    "    display(pd.DataFrame({'y':tokens,\n",
    "                          'yp':list(itertools.chain(*y_pred))}).T)\n",
    "\n",
    "\n",
    "parser = Parser()\n",
    "df = pd.read_csv('ner_corpus.csv',delimiter=',')\n",
    "tokens,labels = make_ner_corpus(parser,df)\n",
    "ldf = pd.DataFrame({'tokens':tokens,'labels':labels})\n",
    "\n",
    "X_vect1,tfidf_vectorizer = tfidf(tokens)\n",
    "X_vect2,dict_vectorizer = dicttransformer(tokens)\n",
    "X_all,model = merger_train(X_vect1,X_vect2,labels)\n",
    "predict_label(X_all,tokens,labels,model)\n",
    "\n",
    "# tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dca6b3bb-3601-4087-8db7-db7d2b570fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>utilising</td>\n",
       "      <td>dataset</td>\n",
       "      <td>a</td>\n",
       "      <td>calculate</td>\n",
       "      <td>the</td>\n",
       "      <td>fourier</td>\n",
       "      <td>transformation</td>\n",
       "      <td>of</td>\n",
       "      <td>columns</td>\n",
       "      <td>b</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yp</th>\n",
       "      <td>B-SOURCE</td>\n",
       "      <td>I-SOURCE</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PARAM</td>\n",
       "      <td>I-PARAM</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1  2          3    4        5               6        7   \\\n",
       "y   utilising   dataset  a  calculate  the  fourier  transformation       of   \n",
       "yp   B-SOURCE  I-SOURCE  O          O    O        O               O  B-PARAM   \n",
       "\n",
       "         8  9  10 11  \n",
       "y   columns  b  c  d  \n",
       "yp  I-PARAM  O  O  O  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Inferences\n",
    "\n",
    "'''\n",
    "\n",
    "# request = \"generate the fourier transformation of column A using data B\"\n",
    "# request = \"generate the fourier transformation of columns A B C utilising data D\"\n",
    "request = \"utilising dataset A calculate the fourier transformation of columns B C D\"\n",
    "request = request.lower()\n",
    "tokens = nltk_wtokeniser(request)\n",
    "\n",
    "X_vect1,_ = tfidf(tokens,tfidf_vectorizer)\n",
    "X_vect2,_ = dicttransformer(tokens,dict_vectorizer)\n",
    "X_all = merger(X_vect1,X_vect2)\n",
    "predict(X_all,tokens,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "224c9a46-a644-4d0c-b185-f50fc2ade443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# '''\n",
    "\n",
    "# Tune CatBoost Model\n",
    "\n",
    "# '''\n",
    "\n",
    "# def tune_model(X,labels):\n",
    "    \n",
    "#     labels = np.array(labels)[:,None]\n",
    "\n",
    "#     # Split the dataset into train and test sets\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X,labels, test_size=0.2, random_state=42)\n",
    "\n",
    "#     # print(X_train.shape)\n",
    "#     # print(X_test.shape)\n",
    "#     # print(y_train.shape)\n",
    "#     # print(y_test.shape)\n",
    "    \n",
    "#     # Define the objective function for Optuna\n",
    "#     def objective(trial):\n",
    "#         params = {\n",
    "#             'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "#             'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n",
    "#             'depth': trial.suggest_int('depth', 3, 10),\n",
    "#             'border_count': trial.suggest_int('border_count', 1, 255),\n",
    "#             'random_seed': 42,\n",
    "#             'loss_function': 'MultiClass',\n",
    "#             'eval_metric': 'Accuracy',\n",
    "#             'verbose': False,\n",
    "#         }\n",
    "        \n",
    "#         # Train the model with the current set of hyperparameters\n",
    "#         model = CatBoostClassifier(**params)\n",
    "#         model.fit(X_train, y_train)\n",
    "        \n",
    "#         # Evaluate the model on the test set\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "#         return accuracy\n",
    "    \n",
    "#     study = optuna.create_study(direction='maximize') # Create an Optuna study\n",
    "#     study.optimize(objective, n_trials=100) # Run the optimization\n",
    "#     best_params = study.best_params; best_accuracy = study.best_value\n",
    "\n",
    "#     return best_params\n",
    "\n",
    "# X_vect1,_ = tfidf(tokens,tfidf_vectorizer)\n",
    "# X_vect2,_ = dicttransformer(tokens,dict_vectorizer)\n",
    "# X_all = merger(X_vect1,X_vect2)\n",
    "# best_params = tune_model(X_all,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa822a45-460c-4b5b-9528-aa90821363c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
