{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c991d33-26f6-40a1-9365-f4bda2437d07",
   "metadata": {},
   "source": [
    "## **Named Entity Recognition** \n",
    "\n",
    "### **NER Parser**\n",
    "\n",
    "Create NER tagger to identify words/tokens of interest in input request, it is used to set parameters & remove irrelovant tokens before feeding the input into the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4131af7-ea30-466c-895e-10ce258e967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import regex as re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "'''\n",
    "\n",
    "PARSER FOR THE DATASET NER TAG FORMAT\n",
    "\n",
    "'''\n",
    "\n",
    "class Parser:\n",
    "    \n",
    "    # RE patterns for tag extraction\n",
    "    LABEL_PATTERN = r\"\\[(.*?)\\]\"\n",
    "    PUNCTUATION_PATTERN = r\"([,\\/#!$%\\^&\\*;:{}=\\-`~()'\\\"’¿])\"\n",
    "    \n",
    "    # initialise, first word/id tag is O (outside)\n",
    "    def __init__(self):\n",
    "        self.tag_to_id = {\n",
    "            \"O\": 0\n",
    "        }\n",
    "        self.id_to_tag = {\n",
    "            0: \"O\"\n",
    "        }\n",
    "        \n",
    "    ''' CREATE TAGS '''\n",
    "        \n",
    "    # input : sentence, tagged sentence\n",
    "        \n",
    "    def __call__(self, sentence: str, annotated: str) -> List[str]:\n",
    "        \n",
    "        ''' Create Dictionary of Identified Tags'''\n",
    "        \n",
    "        # 1. set label B or I    \n",
    "        matches = re.findall(self.LABEL_PATTERN, annotated)\n",
    "        word_to_tag = {}\n",
    "        \n",
    "        for match in matches:            \n",
    "            if(\" : \" in match):\n",
    "                tag, phrase = match.split(\" : \")\n",
    "                words = phrase.split(\" \") \n",
    "                word_to_tag[words[0]] = f\"B-{tag.upper()}\"\n",
    "                for w in words[1:]:\n",
    "                    word_to_tag[w] = f\"I-{tag.upper()}\"\n",
    "                \n",
    "        ''' Tokenise Sentence & add tags to not tagged words (O)'''\n",
    "                \n",
    "        # 2. add token tag to main tag dictionary\n",
    "\n",
    "        tags = []\n",
    "        sentence = re.sub(self.PUNCTUATION_PATTERN, r\" \\1 \", sentence)\n",
    "        \n",
    "        for w in sentence.split():\n",
    "            if w not in word_to_tag:\n",
    "                tags.append(\"O\")\n",
    "            else:\n",
    "                tags.append(word_to_tag[w])\n",
    "                self.__add_tag(word_to_tag[w])\n",
    "                \n",
    "        return tags\n",
    "    \n",
    "    ''' TAG CONVERSION '''\n",
    "    \n",
    "    # to word2id (tag_to_id)\n",
    "    # to id2word (id_to_tag)\n",
    "\n",
    "    def __add_tag(self, tag: str):\n",
    "        if tag in self.tag_to_id:\n",
    "            return\n",
    "        id_ = len(self.tag_to_id)\n",
    "        self.tag_to_id[tag] = id_\n",
    "        self.id_to_tag[id_] = tag\n",
    "        \n",
    "        ''' Get Tag Number ID '''\n",
    "        # or just number id for token\n",
    "        \n",
    "    def get_id(self, tag: str):\n",
    "        return self.tag_to_id[tag]\n",
    "    \n",
    "    ''' Get Tag Token from Number ID'''\n",
    "    # given id get its token\n",
    "    \n",
    "    def get_label(self, id_: int):\n",
    "        return self.get_tag_label(id_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55ee2548-9dda-493a-bdd5-89cd0aa8947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "\n",
    "# NER with Machine Learning Models\n",
    "\n",
    "# '''\n",
    "    \n",
    "# # pattern for tokenisation\n",
    "# PUNCTUATION_PATTERN = r\"([,\\/#!$%\\^&\\*;:{}=\\-`~()'\\\"’¿])\"\n",
    "\n",
    "# # customiser tokeniser\n",
    "# def cust_tokeniser(inputs):\n",
    "#     sentence = re.sub(PUNCTUATION_PATTERN, r\" \\1 \", inputs)\n",
    "#     return sentence.split()\n",
    "\n",
    "# # parser\n",
    "# parser = Parser()\n",
    "# # df = pd.read_csv('ner_modelparams_annot.csv')   # read dataframe\n",
    "# df = pd.read_csv('../src/mllibs/corpus/ner_corpus.csv',delimiter=',')\n",
    "\n",
    "# def make_model(parser,df):\n",
    "\n",
    "#     # parse our NER tag data & tokenise our text\n",
    "#     lst_data = []; lst_tags = []\n",
    "#     for ii,row in df.iterrows():\n",
    "#         sentence = re.sub(PUNCTUATION_PATTERN, r\" \\1 \", row['question'])\n",
    "#         lst_data.extend(sentence.split())\n",
    "#         lst_tags.extend(parser(row[\"question\"], row[\"annotated\"]))\n",
    "    \n",
    "#     ldf = pd.DataFrame({'data':lst_data,\n",
    "#                         'tag':lst_tags})\n",
    "    \n",
    "#     ''' \n",
    "    \n",
    "#     Vectorisation \n",
    "    \n",
    "#     '''\n",
    "        \n",
    "#     # define encoder\n",
    "#     # encoder = CountVectorizer(tokenizer=cust_tokeniser,ngram_range=(1,1))\n",
    "#     encoder = CountVectorizer(tokenizer=cust_tokeniser)\n",
    "#     # encoder = TfidfVectorizer(tokenizer=cust_tokeniser,ngram_range=(1,5))\n",
    "#     X = encoder.fit_transform(lst_data)\n",
    "#     y = np.array(lst_tags)\n",
    "    \n",
    "#     ''' \n",
    "    \n",
    "#     Modeling \n",
    "    \n",
    "#     '''\n",
    "    \n",
    "#     # try our different models\n",
    "#     # model_confirm = LogisticRegression()\n",
    "#     model_confirm = CatBoostClassifier(silent=True)\n",
    "#     # model_confirm = RandomForestClassifier(max_depth=200,min_samples_split=10)\n",
    "    \n",
    "#     # train model\n",
    "#     model_confirm.fit(X,y)\n",
    "#     y_pred = model_confirm.predict(X)\n",
    "#     print(f'accuracy: {round(accuracy_score(y_pred,y),3)}')\n",
    "\n",
    "#     print(classification_report(y, y_pred))\n",
    "#     # display(pd.DataFrame(confusion_matrix(y,y_pred),index=model_confirm.classes_,columns=model_confirm.classes_))\n",
    "#     return model_confirm,encoder\n",
    "\n",
    "# model,encoder = make_model(parser,df)\n",
    "# # df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2843ea96-4a0b-4693-9f00-ab3df268b066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inputs = \"create scatterplot using data and x A y B and hue C\"\n",
    "# # inputs = \"create relplot using data x flow, y length col:A and row D, alpha 0.1\"\n",
    "# # inputs1 = \"create seaborn scatterplot using data penguins x bill_length_mm y bill_depth_mm hue island\"\n",
    "# # inputs2 = \"create seaborn scatterplot using penguins x bill_length_mm y bill_depth_mm hue island\"\n",
    "# # inputs = \"create seaborn scatterplot using data penguins x bill_length_mm y bill_depth_mm hue island select numerical features only\"\n",
    "# # inputs = \"create seaborn scatterplot using data penguins (use numerical columns only) x bill_length_mm y bill_depth_mm hue island\"\n",
    "\n",
    "# import itertools\n",
    "\n",
    "# '''\n",
    "\n",
    "# Implementing references to dataframe subsets\n",
    "\n",
    "# '''\n",
    "\n",
    "# # inputs = \"create label encoding of column B using data A\"     # not ok\n",
    "# # inputs = \"create label encoding for column B using data A\"    # not ok\n",
    "# # inputs = \"create one hot encoding of columns A B C using data E\" # ok\n",
    "# # inputs = \"create label encoding using active columns C from data E\"\n",
    "# # inputs = \"show the unique values in columns island in data penguins\"\n",
    "# request = \"how many rows are missing in data titanic, in terms of percentage\"\n",
    "\n",
    "\n",
    "# # predict NER tags\n",
    "# def ner_predict(inputs):\n",
    "#     # tokens = word_tokenize(inputs)\n",
    "#     tokens = cust_tokeniser(inputs)\n",
    "#     y_pred_test = model.predict(encoder.transform(tokens))\n",
    "#     y_pred_test = list(itertools.chain.from_iterable(y_pred_test))\n",
    "#     return pd.DataFrame({\"input\":tokens,\n",
    "#                          \"pred\":y_pred_test})\n",
    "\n",
    "\n",
    "# outputs = ner_predict(inputs)\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "61c429af-b33a-443a-8888-f1edb77991b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk import pos_tag\n",
    "# from nltk.chunk import ne_chunk\n",
    "# from nltk.corpus import treebank\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# # Sample text data\n",
    "# text = \"Apple is planning to open a new store in San Francisco next month.\"\n",
    "\n",
    "# # Tokenize the input text\n",
    "# tokens = word_tokenize(text)\n",
    "\n",
    "# # Perform part-of-speech tagging\n",
    "# pos_tags = pos_tag(tokens)\n",
    "\n",
    "# # Define a simple rule-based named entity recognition function\n",
    "# def extract_named_entities(pos_tags):\n",
    "#     named_entities = []\n",
    "#     for chunk in ne_chunk(pos_tags):\n",
    "#         if hasattr(chunk, 'label'):\n",
    "#             entity = ' '.join(c[0] for c in chunk)\n",
    "#             named_entities.append((entity, chunk.label()))\n",
    "#     return named_entities\n",
    "\n",
    "# # Extract named entities from the text using the rule-based function\n",
    "# named_entities = extract_named_entities(pos_tags)\n",
    "\n",
    "# # Define features for each word/token\n",
    "# def word_features(word, index, tokens):\n",
    "#     return {\n",
    "#         'word': word,\n",
    "#         'is_first': index == 0,\n",
    "#         'is_last': index == len(tokens) - 1,\n",
    "#         'is_title': word.istitle(),\n",
    "#         'is_upper': word.isupper(),\n",
    "#         'is_lower': word.islower(),\n",
    "#         'prefix-1': word[0],\n",
    "#         'prefix-2': word[:2],\n",
    "#         'suffix-1': word[-1],\n",
    "#         'suffix-2': word[-2:],\n",
    "#     }\n",
    "\n",
    "# # Extract features for each token in the text\n",
    "# features = [word_features(token, i, tokens) for i, token in enumerate(tokens)]\n",
    "\n",
    "# # Convert features to a sparse matrix using DictVectorizer\n",
    "# vectorizer = DictVectorizer(sparse=True)\n",
    "# X = vectorizer.fit_transform(features)\n",
    "\n",
    "# # Define labeled named entities for training\n",
    "# y = ['O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'B-DATE']\n",
    "\n",
    "# # Train a Random Forest model\n",
    "# clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# clf.fit(X, y)\n",
    "\n",
    "# # New test text for prediction\n",
    "# test_text = \"Microsoft is also considering a new office in Seattle next year.\"\n",
    "\n",
    "# # Tokenize and extract features for the test text\n",
    "# test_tokens = word_tokenize(test_text)\n",
    "# test_features = [word_features(token, i, test_tokens) for i, token in enumerate(test_tokens)]\n",
    "\n",
    "# # Use the same DictVectorizer object to transform the test features\n",
    "# X_test = vectorizer.transform(test_features)\n",
    "\n",
    "# # Make predictions using the trained Random Forest model\n",
    "# predictions = clf.predict(X_test)\n",
    "\n",
    "# # Print the predicted named entities\n",
    "# print(list(zip(test_tokens, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "663aec6f-7737-46a8-ae05-826024821ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Sample data\n",
    "# data = {\n",
    "#     'text': ['Apple is a company', 'New York is a city', 'John works at Google'],\n",
    "#     'label': ['ORG', 'LOC', 'PER']\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Feature extraction\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(df['text'])\n",
    "# y = df['label']\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train the Random Forest classifier\n",
    "# clf = RandomForestClassifier()\n",
    "# clf.fit(X, y)\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred = clf.predict(X)\n",
    "# y_pred\n",
    "\n",
    "# # Evaluate the model\n",
    "# # print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "228ae0c4-927b-4ba9-b56c-cbfea825305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "\n",
    "# Use Transformer Embedding\n",
    "\n",
    "# '''\n",
    "\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load pre-trained BERT model and tokenizer\n",
    "# model_name = \"bert-base-uncased\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# # Example text and corresponding labels\n",
    "# texts = [\"Apple is a company based in California.\", \"Python is a programming language.\"]\n",
    "# labels = [\"ORG\", \"MISC\"]\n",
    "\n",
    "# # Tokenize and encode the texts\n",
    "# encoded_texts = [tokenizer.encode(text, return_tensors=\"pt\", padding=True, truncation=True) for text in texts]\n",
    "\n",
    "# # Generate BERT embeddings for the encoded texts\n",
    "# with torch.no_grad():\n",
    "#     outputs = [model(input_ids).last_hidden_state.mean(dim=1).numpy() for input_ids in encoded_texts]\n",
    "\n",
    "# # Flatten the embeddings and labels for training\n",
    "# X = np.concatenate(outputs, axis=0)\n",
    "# y = labels\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train a Random Forest classifier\n",
    "# clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the classifier\n",
    "# accuracy = clf.score(X_test, y_test)\n",
    "# print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "22936391-1c31-4d24-ad50-cbeafb778bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk import word_tokenize\n",
    "# from nltk.util import ngrams\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# # Sample dataset with labeled texts\n",
    "# texts = [\n",
    "#     \"Apple is a company based in California.\",\n",
    "#     \"Microsoft is known for its Windows operating system.\",\n",
    "#     \"Python is a popular programming language.\",\n",
    "#     \"The Eiffel Tower is located in Paris, France.\"\n",
    "# ]\n",
    "# labels = [\"ORG\", \"ORG\", \"MISC\", \"LOC\"]\n",
    "\n",
    "# # Function to extract n-gram features from the input text\n",
    "# def extract_ngram_features(text, n):\n",
    "#     tokens = word_tokenize(text)\n",
    "#     ngram_features = ngrams(tokens, n)\n",
    "#     return [' '.join(gram) for gram in ngram_features]\n",
    "\n",
    "# # Extract n-gram features for each text\n",
    "# n = 2  # Using bigram features\n",
    "# ngram_features = [extract_ngram_features(text, n) for text in texts]\n",
    "\n",
    "# # Convert n-gram features to dictionary format for vectorization\n",
    "# ngram_features_dict = [{feature: 1 for feature in features} for features in ngram_features]\n",
    "\n",
    "# # Vectorize the n-gram features\n",
    "# vectorizer = DictVectorizer()\n",
    "# X = vectorizer.fit_transform(ngram_features_dict)\n",
    "# y = labels\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train a Random Forest classifier\n",
    "# clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# clf.fit(X, y)\n",
    "\n",
    "# # Predict labels for the test set\n",
    "# y_pred = clf.predict(X)\n",
    "\n",
    "# # Evaluate the classifier\n",
    "# accuracy = accuracy_score(y, y_pred)\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# # Print classification report\n",
    "# print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "34bf19c5-8a9e-46ca-9300-5137da061737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk import word_tokenize\n",
    "# from nltk.util import ngrams\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# # Sample dataset with labeled texts\n",
    "# texts = [\n",
    "#     \"Apple is a company based in California.\",\n",
    "#     \"Microsoft is known for its Windows operating system.\",\n",
    "#     \"Python is a popular programming language.\",\n",
    "#     \"The Eiffel Tower is located in Paris, France.\"\n",
    "# ]\n",
    "# labels = [\"ORG\", \"ORG\", \"MISC\", \"LOC\"]\n",
    "\n",
    "# # Function to extract n-gram features from the input text\n",
    "# def extract_ngram_features(text, n):\n",
    "#     tokens = word_tokenize(text)\n",
    "#     ngram_features = ngrams(tokens, n)\n",
    "#     return [' '.join(gram) for gram in ngram_features]\n",
    "\n",
    "# # Extract unigram and bigram features for each text\n",
    "# unigram_features = [word_tokenize(text) for text in texts]\n",
    "# bigram_features = [extract_ngram_features(text, 2) for text in texts]\n",
    "\n",
    "# # # Convert unigram and bigram features to dictionary format for vectorization\n",
    "# unigram_features_dict = [{feature: 1 for feature in features} for features in unigram_features]\n",
    "# bigram_features_dict = [{feature: 1 for feature in features} for features in bigram_features]\n",
    "\n",
    "# # # Combine unigram and bigram features\n",
    "# combined_features_dict = [{**uni, **bi} for uni, bi in zip(unigram_features_dict, bigram_features_dict)]\n",
    "# combined_features_dict\n",
    "\n",
    "# # # Vectorize the combined features\n",
    "# # vectorizer = DictVectorizer()\n",
    "# # X = vectorizer.fit_transform(combined_features_dict)\n",
    "# # print(X.shape)\n",
    "\n",
    "# # # Split the data into training and testing sets\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # # Train a Random Forest classifier\n",
    "# # clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# # clf.fit(X_train, y_train)\n",
    "\n",
    "# # # Predict labels for the test set\n",
    "# # y_pred = clf.predict(X_test)\n",
    "\n",
    "# # # Evaluate the classifier\n",
    "# # accuracy = accuracy_score(y_test, y_pred)\n",
    "# # print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# # # Print classification report\n",
    "# # print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa2bb57b-88e8-4921-927c-72f119bb4384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import pandas as pd    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "'''\n",
    "\n",
    "PARSER FOR THE DATASET NER TAG FORMAT\n",
    "\n",
    "'''\n",
    "\n",
    "# Tokenisation patten\n",
    "PUNCTUATION_PATTERN = r\"([,\\/#!$%\\^&\\*;:{}=\\-`~()'\\\"’¿])\"\n",
    "# RE patterns for tag extraction\n",
    "LABEL_PATTERN = r\"\\[(.*?)\\]\"\n",
    "\n",
    "class Parser:\n",
    "    \n",
    "    # initialise, first word/id tag is O (outside)\n",
    "    def __init__(self):\n",
    "        self.tag_to_id = {\"O\": 0}\n",
    "        self.id_to_tag = {0: \"O\"}\n",
    "        \n",
    "    ''' CREATE TAGS '''\n",
    "        \n",
    "    # input : sentence, tagged sentence\n",
    "        \n",
    "    def __call__(self, sentence: str, annotated: str) -> List[str]:\n",
    "        \n",
    "        ''' Create Dictionary of Identified Tags'''\n",
    "        \n",
    "        # 1. set label B or I    \n",
    "        matches = re.findall(LABEL_PATTERN, annotated)\n",
    "        word_to_tag = {}\n",
    "        \n",
    "        for match in matches:            \n",
    "            if(\" : \" in match):\n",
    "                tag, phrase = match.split(\" : \")\n",
    "                words = phrase.split(\" \") \n",
    "                word_to_tag[words[0]] = f\"B-{tag.upper()}\"\n",
    "                for w in words[1:]:\n",
    "                    word_to_tag[w] = f\"I-{tag.upper()}\"\n",
    "                \n",
    "        ''' Tokenise Sentence & add tags to not tagged words (O)'''\n",
    "                \n",
    "        # 2. add token tag to main tag dictionary\n",
    "\n",
    "        tags = []\n",
    "        sentence = re.sub(PUNCTUATION_PATTERN, r\" \\1 \", sentence)\n",
    "        \n",
    "        for w in sentence.split():\n",
    "            if w not in word_to_tag:\n",
    "                tags.append(\"O\")\n",
    "            else:\n",
    "                tags.append(word_to_tag[w])\n",
    "                self.__add_tag(word_to_tag[w])\n",
    "                \n",
    "        return tags\n",
    "    \n",
    "    ''' TAG CONVERSION '''\n",
    "    \n",
    "    # to word2id (tag_to_id)\n",
    "    # to id2word (id_to_tag)\n",
    "\n",
    "    def __add_tag(self, tag: str):\n",
    "        if tag in self.tag_to_id:\n",
    "            return\n",
    "        id_ = len(self.tag_to_id)\n",
    "        self.tag_to_id[tag] = id_\n",
    "        self.id_to_tag[id_] = tag\n",
    "        \n",
    "        ''' Get Tag Number ID '''\n",
    "        # or just number id for token\n",
    "        \n",
    "    def get_id(self, tag: str):\n",
    "        return self.tag_to_id[tag]\n",
    "    \n",
    "    ''' Get Tag Token from Number ID'''\n",
    "    # given id get its token\n",
    "    \n",
    "    def get_label(self, id_: int):\n",
    "        return self.get_tag_label(id_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f9c8e86-3128-4685-9794-547e1c6130b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier_subset.csv      ner_mp.csv\n",
      "generative_corpus.csv      reference_database.csv\n",
      "ner_corpus.csv             wordlist.10000.txt\n",
      "ner_modelparams_annot.csv\n"
     ]
    }
   ],
   "source": [
    "ls ../src/mllibs/corpus/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0ef03548-5915-49fc-909a-18925f50d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Create NER corpus\n",
    "\n",
    "'''\n",
    "\n",
    "from nltk.tokenize import word_tokenize, WhitespaceTokenizer \n",
    "\n",
    "def make_ner_corpus(parser,df:pd.DataFrame):\n",
    "\n",
    "    # parse our NER tag data & tokenise our text\n",
    "    lst_data = []; lst_data_nested = []; lst_tags = []\n",
    "    for ii,row in df.iterrows():\n",
    "        sentence = re.sub(PUNCTUATION_PATTERN, r\" \\1 \", row['question'])\n",
    "        lst_data_nested.append(sentence.split())\n",
    "        lst_data.extend(sentence.split())\n",
    "        lst_tags.extend(parser(row[\"question\"], row[\"annotated\"]))\n",
    "        \n",
    "    return lst_data_nested,lst_data,lst_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32357cb8-7154-4b60-be77-9f2e690f8985",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Full Variant\n",
    "\n",
    "'''\n",
    "\n",
    "# for each token list create features\n",
    "def extract_token_features2(tokens: list):\n",
    "    \n",
    "    token_features = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        features = {\n",
    "            'token': token,\n",
    "            'is_first_token': i == 0,\n",
    "            'is_last_token': i == len(tokens) - 1,\n",
    "            'is_capitalized': token[0].isupper(),\n",
    "            'is_alphanumeric': token.isalnum(),\n",
    "        }\n",
    "\n",
    "        if i < len(tokens) - 1:\n",
    "            next_token = tokens[i+1]\n",
    "            features['next_token_p1'] = next_token\n",
    "            features['is_next_first_token_p1'] = i + 1 == 0\n",
    "            features['is_next_last_token_p1'] = i + 1 == len(tokens) - 2\n",
    "            features['is_next_numeric_p1'] = next_token.isdigit()\n",
    "            features['is_next_alphanumeric_p1'] = next_token.isalnum()\n",
    "        else:\n",
    "            features['next_token_p1'] = \"None\"\n",
    "            features['is_next_first_token_p1'] = \"None\"\n",
    "            features['is_next_last_token_p1'] = \"None\"\n",
    "            features['is_next_numeric_p1'] = \"None\"\n",
    "            features['is_next_alphanumeric_p1'] = \"None\"\n",
    "        \n",
    "        if i > 1:\n",
    "            prev_token = tokens[i-1]\n",
    "            features['prev_token_m1'] = prev_token\n",
    "            features['is_prev_first_token_m1'] = i - 1 == 0\n",
    "            features['is_prev_last_token_m1'] = i - 1 == len(tokens) - 2\n",
    "            features['is_prev_numeric_m1'] = prev_token.isdigit()\n",
    "            features['is_prev_alphanumeric_m1'] = prev_token.isalnum()\n",
    "            \n",
    "        else:\n",
    "            features['prev_token_m1'] = \"None\"\n",
    "            features['is_prev_first_token_m1'] = \"None\"\n",
    "            features['is_prev_last_token_m1'] = \"None\"\n",
    "            features['is_prev_numeric_m1'] = \"None\"\n",
    "            features['is_prev_alphanumeric_m1'] = \"None\"\n",
    "\n",
    "        if i < len(tokens) - 2:\n",
    "            next_token = tokens[i+2]\n",
    "            features['next_token_p2'] = next_token\n",
    "            features['is_next_first_token_p2'] = i + 1 == 0\n",
    "            features['is_next_last_token_p2'] = i + 1 == len(tokens) - 2\n",
    "            features['is_next_numeric_p2'] = next_token.isdigit()\n",
    "            features['is_next_alphanumeric_p2'] = next_token.isalnum()\n",
    "        else:\n",
    "            features['next_token_p2'] = \"None\"\n",
    "            features['is_next_first_token_p2'] = \"None\"\n",
    "            features['is_next_last_token_p2'] = \"None\"\n",
    "            features['is_next_numeric_p2'] = \"None\"\n",
    "            features['is_next_alphanumeric_p2'] = \"None\"\n",
    "\n",
    "        if i > 2:\n",
    "            prev_token = tokens[i-2]\n",
    "            features['prev_token_m2'] = prev_token\n",
    "            features['is_prev_first_token_m2'] = i - 1 == 0\n",
    "            features['is_prev_last_token_m2'] = i - 1 == len(tokens) - 2\n",
    "            features['is_prev_numeric_m2'] = prev_token.isdigit()\n",
    "            features['is_prev_alphanumeric_m2'] = prev_token.isalnum()\n",
    "        else:\n",
    "            features['prev_token_m2'] = \"None\"\n",
    "            features['is_prev_first_token_m2'] = \"None\"\n",
    "            features['is_prev_last_token_m2'] = \"None\"\n",
    "            features['is_prev_numeric_m2'] = \"None\"\n",
    "            features['is_prev_alphanumeric_m2'] = \"None\"\n",
    "\n",
    "        token_features.append(features)\n",
    "        \n",
    "    return token_features\n",
    "\n",
    "'''\n",
    "\n",
    "Smaller Variant\n",
    "\n",
    "'''\n",
    "\n",
    "def extract_token_features(tokens:list):\n",
    "    \n",
    "    token_features = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        features = {\n",
    "            'token': token,\n",
    "            'is_first_token': i == 0,\n",
    "            'is_last_token': i == len(tokens) - 1,\n",
    "            'is_capitalized': token[0].isupper(),\n",
    "            'is_all_caps': token.isupper(),\n",
    "            'is_numeric': token.isdigit(),\n",
    "            'is_alphanumeric': token.isalnum(),\n",
    "            'is_punctuation': token in punctuation\n",
    "        }\n",
    "        token_features.append(features)\n",
    "        \n",
    "    return token_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c682a22-4f74-44d1-9688-974be7e7cd08",
   "metadata": {},
   "source": [
    "## **Training Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d47ddfd5-b6c5-4c48-a855-031b53551cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "824\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 148\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# ldf = pd.DataFrame({'tokens':tokens,'labels':labels})\u001b[39;00m\n\u001b[1;32m    147\u001b[0m X_vect1,tfidf_vectorizer \u001b[38;5;241m=\u001b[39m tfidf_train(tokens)\n\u001b[0;32m--> 148\u001b[0m X_vect2,dict_vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mdicttransformer_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m X_all,model \u001b[38;5;241m=\u001b[39m merger_train(X_vect1,X_vect2,labels)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# predict_label(X_all,tokens,labels,model)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[68], line 58\u001b[0m, in \u001b[0;36mdicttransformer_train\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     55\u001b[0m token_features \u001b[38;5;241m=\u001b[39m extract_token_features2(tokens)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# token_features = extract_token_features(tokens)\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtoken_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Vectorize the token features\u001b[39;00m\n\u001b[1;32m     61\u001b[0m vectoriser \u001b[38;5;241m=\u001b[39m DictVectorizer()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from string import punctuation\n",
    "from catboost import CatBoostClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "'''\n",
    "############################################################\n",
    "\n",
    "tf-idf transformer approach to NER\n",
    "\n",
    "        need to tokenise first; use whitespace tokeniser\n",
    "        so its the same as the dicttransformer\n",
    "\n",
    "############################################################\n",
    "'''\n",
    "\n",
    "from nltk.tokenize import word_tokenize, WhitespaceTokenizer \n",
    "\n",
    "def nltk_wtokeniser(text):\n",
    "    return WhitespaceTokenizer().tokenize(text)\n",
    "\n",
    "def tfidf_train(tokens:list):\n",
    "    \n",
    "    vectoriser = TfidfVectorizer(tokenizer=lambda x: nltk_wtokeniser(x),token_pattern=None)\n",
    "    # vectoriser = CountVectorizer()\n",
    "    X = vectoriser.fit_transform(tokens)\n",
    "    return X,vectoriser\n",
    "\n",
    "def tfidf_transform(tokens:list,vectoriser):\n",
    "    X = vectoriser.transform(tokens)\n",
    "    return X\n",
    "\n",
    "'''\n",
    "############################################################\n",
    "\n",
    "dicttransformers approach to NER\n",
    "\n",
    "        created for each token in list\n",
    "\n",
    "############################################################\n",
    "'''\n",
    "\n",
    "# tokens : nested list for each training document\n",
    "def dicttransformer_train(nested_tokens:list[list]):\n",
    "\n",
    "    all_features = []\n",
    "    for tokens in nested_tokens\n",
    "    \n",
    "        # Extract token-level features for each token\n",
    "        token_features = extract_token_features2(tokens)\n",
    "        # token_features = extract_token_features(tokens)\n",
    "        all_features.extend(token_features)\n",
    "    \n",
    "    \n",
    "    # Vectorize the token features\n",
    "    vectoriser = DictVectorizer()\n",
    "    X = vectoriser.fit_transform(all_features) # also sparse\n",
    "    return X,vectoriser\n",
    "        \n",
    "\n",
    "def dicttransformer_transform(tokens:list,vectoriser):\n",
    "\n",
    "    # Extract token-level features for each token\n",
    "    token_features = extract_token_features2(tokens)\n",
    "    # token_features = extract_token_features(tokens)\n",
    "    \n",
    "    X = vectoriser.transform(token_features) # also sparse\n",
    "    return X\n",
    "\n",
    "'''\n",
    "\n",
    "Merge and Predict\n",
    "\n",
    "'''\n",
    "\n",
    "# merge tf-idf & dict features & train model\n",
    "def merger_train(X1,X2,y):\n",
    "\n",
    "    # convert to non-sparse \n",
    "    X_vect1 = pd.DataFrame(np.asarray(X1.todense()))\n",
    "    X_vect2 = pd.DataFrame(np.asarray(X2.todense()))\n",
    "    data = pd.concat([X_vect1,X_vect2],axis=1)\n",
    "    data = data.values\n",
    "\n",
    "    model = CatBoostClassifier(silent=True)\n",
    "    # model = LogisticRegression()\n",
    "    # model = RandomForestClassifier()\n",
    "    model.fit(data,y)\n",
    "    return data,model\n",
    "\n",
    "# merge tf-idf & dict features & train model\n",
    "def merger(X1,X2):\n",
    "\n",
    "    # convert to non-sparse \n",
    "    X_vect1 = pd.DataFrame(np.asarray(X1.todense()))\n",
    "    X_vect2 = pd.DataFrame(np.asarray(X2.todense()))\n",
    "    data = pd.concat([X_vect1,X_vect2],axis=1)\n",
    "    data = data.values # convert to numpy\n",
    "\n",
    "    return data\n",
    "\n",
    "# predict & measure metric\n",
    "def predict_label(X,tokens,labels,model):\n",
    "    y_pred = model.predict(X)\n",
    "    accuracy = accuracy_score(labels, y_pred)\n",
    "    print(f'accuracy: {round(accuracy_score(y_pred,labels),3)}')\n",
    "    print(classification_report(labels, y_pred))\n",
    "    print(confusion_matrix(labels,y_pred))\n",
    "    # display(pd.DataFrame({'y':tokens,\n",
    "    #                       'yp':list(itertools.chain(*y_pred))}).T)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def predict_label(X, tokens, labels, model):\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    mispredictions = []\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] != labels[i]:\n",
    "            mispredictions.append((tokens[i], labels[i], y_pred[i]))\n",
    "    \n",
    "    accuracy = accuracy_score(labels, y_pred)\n",
    "    print(f'accuracy: {round(accuracy_score(y_pred, labels), 3)}')\n",
    "    print(classification_report(labels, y_pred))\n",
    "    print(confusion_matrix(labels, y_pred))\n",
    "    return mispredictions\n",
    "\n",
    "# just predict (inference)\n",
    "def predict(X,tokens,model):\n",
    "    y_pred = model.predict(X)\n",
    "    display(pd.DataFrame({'y':tokens,\n",
    "                          'yp':list(itertools.chain(*y_pred))}).T)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "parser = Parser()\n",
    "# df = pd.read_csv('ner_corpus.csv',delimiter=',')\n",
    "df = pd.read_csv('../src/mllibs/corpus/ner_corpus.csv',delimiter=',')\n",
    "tokens_nested, tokens,labels = make_ner_corpus(parser,df)\n",
    "ldf = pd.DataFrame({'tokens':tokens,'labels':labels})\n",
    "\n",
    "X_vect1,tfidf_vectorizer = tfidf_train(tokens)\n",
    "X_vect2,dict_vectorizer = dicttransformer_train(tokens_nested)\n",
    "X_all,model = merger_train(X_vect1,X_vect2,labels)\n",
    "# predict_label(X_all,tokens,labels,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2d38e89d-deb3-428c-a3e3-390eb94b2ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0.1', '1', '1.4', '1.5', '10', '100', '2', '3', '30', '4', '5',\n",
       "       '6', '7', '8', '9', 'a', 'all', 'alpha', 'and', 'are', 'as',\n",
       "       'author', 'axis', 'b', 'bag', 'bill_depth_mm', 'bill_length_mm',\n",
       "       'black', 'body_mass_g', 'box', 'boxplot', 'bw', 'c', 'calculate',\n",
       "       'col', 'col_wrap', 'column', 'columna', 'columnb', 'columns',\n",
       "       'compare', 'concatenate', 'create', 'd', 'data', 'data2',\n",
       "       'dataframes', 'dataplot', 'dataset', 'dbscan_labels', 'deck',\n",
       "       'define', 'defined', 'defining', 'density', 'diab',\n",
       "       'dimensionality', 'distribution', 'fast', 'fft', 'fill', 'find',\n",
       "       'flipper_length_mm', 'for', 'fourier', 'generate', 'geometry',\n",
       "       'hf', 'histogram', 'housing', 'how', 'hue', 'in', 'inner',\n",
       "       'iqr_labels', 'island', 'join', 'kdeplot', 'kernel', 'lineplot',\n",
       "       'make', 'many', 'marginal', 'marginal_x', 'marginal_y',\n",
       "       'mass_flux', 'mec', 'mew', 'missing', 'model', 'nbins', 'nbinsx',\n",
       "       'nbinsy', 'ngram_range', 'normal', 'of', 'ols', 'outlier_dbscan',\n",
       "       'outlier_iqr', 'outlier_zscore', 'outliers', 'pairplot',\n",
       "       'parameters', 'paramters', 'pca', 'penguins', 'percentage', 'plot',\n",
       "       'plotly', 'plotly_white', 'popmean', 'pressure', 'reduction',\n",
       "       'regression', 'relplot', 'residual', 'result', 'review', 'reviews',\n",
       "       'reviews_data', 'row', 'rows', 's', 's1', 's2', 's3', 's5',\n",
       "       'scatter', 'scatterplot', 'seaborn', 'set', 'setting', 'sex',\n",
       "       'show', 'species', 'stheme', 'store', 'template', 'terms', 'text',\n",
       "       'tfidf', 'the', 'titanic', 'transformation', 'transformations',\n",
       "       'trendline', 'true', 'two', 'use', 'using', 'utilising', 'violin',\n",
       "       'violinplot', 'viridis', 'visualise', 'with', 'words', 'x',\n",
       "       'x_e_out', 'y', 'zscore_labels'], dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf_vectorizer.vocabulary_\n",
    "tfidf_vectorizer.get_feature_names_out()\n",
    "# tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bcbd54-ada6-4684-833e-f7f7538ff681",
   "metadata": {},
   "source": [
    "## **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c175c18c-05c2-4551-9927-75779443ead4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>show</td>\n",
       "      <td>me</td>\n",
       "      <td>the</td>\n",
       "      <td>column</td>\n",
       "      <td>distribution</td>\n",
       "      <td>of</td>\n",
       "      <td>columna</td>\n",
       "      <td>store</td>\n",
       "      <td>the</td>\n",
       "      <td>result</td>\n",
       "      <td>as</td>\n",
       "      <td>data2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yp</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>I-RESTORE</td>\n",
       "      <td>I-RESTORE</td>\n",
       "      <td>I-RESTORE</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1    2       3             4   5        6      7          8   \\\n",
       "y   show  me  the  column  distribution  of  columna  store        the   \n",
       "yp     O   O    O       O             O   O        O      O  I-RESTORE   \n",
       "\n",
       "           9          10     11  \n",
       "y      result         as  data2  \n",
       "yp  I-RESTORE  I-RESTORE      O  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# request = \"generate the fourier transformation of column A using data B\"\n",
    "# request = \"generate the fourier transformation of columns A B C utilising data D\"\n",
    "# request = \"utilising dataset A calculate the fourier transformation of columns B C D\"\n",
    "# request = \"create a bag of words model for text column sex using data penguins ngram_range 1 2\"\n",
    "# request = \"create plotly scatterplot x body_mass_g y bill_length_mm bill_depth_mm using penguins\"\n",
    "# request = \"create plotly scatterplot x body_mass_g y bill_length_mm bill_depth_mm using dataset penguins set mew as 1.5\"\n",
    "# request = \"create plotly scatterplot x body_mass_g y bill_length_mm bill_depth_mm using penguins set parameters mew 1.5\"\n",
    "# request = \"create plotly scatterplot x body_mass_g y bill_length_mm bill_depth_mm using dataset penguins mew 1.5\"\n",
    "# request = \"define parameters mew 1.5 create plotly scatterplot x body_mass_g y bill_length_mm bill_depth_mm using dataset penguins\"\n",
    "# request = \"how many rows are missing in data titanic in terms of percentage\"\n",
    "# request = \"show the distribution of column deck in data titanic store result as data2\"\n",
    "request = \"show me the column distribution of columnA store the result as data2\"\n",
    "# request = \"show the distribution of column deck in data titanic store result data2\"\n",
    "\n",
    "request = request.lower()\n",
    "tokens = nltk_wtokeniser(request)\n",
    "\n",
    "X_vect1 = tfidf_transform(tokens,tfidf_vectorizer)\n",
    "X_vect2 = dicttransformer_transform(tokens,dict_vectorizer)\n",
    "X_all = merger(X_vect1,X_vect2)\n",
    "predict(X_all,tokens,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f095d33f-84ed-4283-950f-4913e67234b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>show me</td>\n",
       "      <td>me the</td>\n",
       "      <td>the column</td>\n",
       "      <td>column distribution</td>\n",
       "      <td>distribution of</td>\n",
       "      <td>of columna</td>\n",
       "      <td>columna store</td>\n",
       "      <td>store the</td>\n",
       "      <td>the result</td>\n",
       "      <td>result as</td>\n",
       "      <td>as data2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yp</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1           2                    3                4   \\\n",
       "y   show me  me the  the column  column distribution  distribution of   \n",
       "yp        O       O           O                    O                O   \n",
       "\n",
       "            5              6          7           8          9         10  \n",
       "y   of columna  columna store  store the  the result  result as  as data2  \n",
       "yp           O              O          O           O          O         O  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def make_ngram_tokens(tokens,n):\n",
    "    ngram_tokens = list(ngrams(tokens, n))\n",
    "    merged_list = [' '.join(t) for t in ngram_tokens]\n",
    "    return merged_list\n",
    "\n",
    "tokens_2 = make_ngram_tokens(tokens,2)\n",
    "X_vect1,_ = tfidf(tokens_2,tfidf_vectorizer)\n",
    "X_vect2,_ = dicttransformer(tokens_2,dict_vectorizer)\n",
    "X_all = merger(X_vect1,X_vect2)\n",
    "predict(X_all,tokens_2,model)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "224c9a46-a644-4d0c-b185-f50fc2ade443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# '''\n",
    "\n",
    "# Tune CatBoost Model\n",
    "\n",
    "# '''\n",
    "\n",
    "# def tune_model(X,labels):\n",
    "    \n",
    "#     labels = np.array(labels)[:,None]\n",
    "\n",
    "#     # Split the dataset into train and test sets\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X,labels, test_size=0.2, random_state=42)\n",
    "\n",
    "#     # print(X_train.shape)\n",
    "#     # print(X_test.shape)\n",
    "#     # print(y_train.shape)\n",
    "#     # print(y_test.shape)\n",
    "    \n",
    "#     # Define the objective function for Optuna\n",
    "#     def objective(trial):\n",
    "#         params = {\n",
    "#             'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "#             'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n",
    "#             'depth': trial.suggest_int('depth', 3, 10),\n",
    "#             'border_count': trial.suggest_int('border_count', 1, 255),\n",
    "#             'random_seed': 42,\n",
    "#             'loss_function': 'MultiClass',\n",
    "#             'eval_metric': 'Accuracy',\n",
    "#             'verbose': False,\n",
    "#         }\n",
    "        \n",
    "#         # Train the model with the current set of hyperparameters\n",
    "#         model = CatBoostClassifier(**params)\n",
    "#         model.fit(X_train, y_train)\n",
    "        \n",
    "#         # Evaluate the model on the test set\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "#         return accuracy\n",
    "    \n",
    "#     study = optuna.create_study(direction='maximize') # Create an Optuna study\n",
    "#     study.optimize(objective, n_trials=100) # Run the optimization\n",
    "#     best_params = study.best_params; best_accuracy = study.best_value\n",
    "\n",
    "#     return best_params\n",
    "\n",
    "# X_vect1,_ = tfidf(tokens,tfidf_vectorizer)\n",
    "# X_vect2,_ = dicttransformer(tokens,dict_vectorizer)\n",
    "# X_all = merger(X_vect1,X_vect2)\n",
    "# best_params = tune_model(X_all,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa822a45-460c-4b5b-9528-aa90821363c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deep learning' 'language processing' 'love deep' 'love machine'\n",
      " 'love natural' 'machine learning' 'natural language']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example list of text documents\n",
    "documents = [\n",
    "    \"I love natural language processing!\",\n",
    "    \"I love machine learning!\",\n",
    "    \"I love deep learning!\"\n",
    "]\n",
    "\n",
    "# Create an instance of CountVectorizer with n-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (n-grams)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the feature names\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87b2ab-4c9e-4af6-be3a-37742fb7a32b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
